{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59e7618-c5e3-4340-81e4-7b8f10919bd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.51.0)\n",
      "Collecting google-cloud-pipeline-components\n",
      "  Downloading google_cloud_pipeline_components-2.14.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: kfp in /opt/conda/lib/python3.10/site-packages (2.5.0)\n",
      "Collecting kfp\n",
      "  Downloading kfp-2.7.0.tar.gz (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.22.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.15)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components) (3.1.4)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (8.1.7)\n",
      "Collecting kfp-pipeline-spec==0.3.0 (from kfp)\n",
      "  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (26.1.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 (from google-cloud-aiplatform)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp) (1.26.18)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components) (2.1.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp) (2024.2.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (69.5.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.11.0)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp) (3.2.2)\n",
      "Downloading google_cloud_pipeline_components-2.14.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610419 sha256=89943d6d82eced8942e29c6abafc918ee45b1d45794099f333f030c3ffe015ba\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n",
      "Successfully built kfp\n",
      "Installing collected packages: protobuf, kfp-pipeline-spec, google-api-core, kfp, google-cloud-pipeline-components\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-deprecated and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.19.0 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.19.0 google-cloud-pipeline-components-2.14.1 kfp-2.7.0 kfp-pipeline-spec-0.3.0 protobuf-4.25.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform \\\n",
    "                                google-cloud-pipeline-components \\\n",
    "                                kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f6aa75-a70a-4355-af77-ed87214b7762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66879c36-791b-459e-ac9b-f3101c6e5225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(dotenv_path=\"variables.env\")\n",
    "# env_vars = dotenv_values(\"variables.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8375f5-5da2-42d4-b727-a6fa26ada13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "_project = !gcloud config list project --format \"value(core.project)\"\n",
    "PROJECT = _project[0]\n",
    "\n",
    "LOCATION = os.environ[\"LOCATION\"]\n",
    "\n",
    "# bucket names\n",
    "DATA_BUCKET_NAME = os.environ[\"DATA_BUCKET_NAME\"]\n",
    "PROCESSED_DATA_SAVE_BUCKET_NAME = os.environ[\"PROCESSED_DATA_SAVE_BUCKET_NAME\"]\n",
    "PROCESSED_DATA_BUCKET_NAME = os.environ[\"PROCESSED_DATA_BUCKET_NAME\"]\n",
    "NEW_TRAIN_DATA_BUCKET_NAME = os.environ[\"NEW_TRAIN_DATA_BUCKET_NAME\"]\n",
    "VALID_DATA_BUCKET_NAME = os.environ[\"VALID_DATA_BUCKET_NAME\"]\n",
    "\n",
    "MODEL_BUCKET_NAME = os.environ[\"MODEL_BUCKET_NAME\"]\n",
    "\n",
    "# bucket uris\n",
    "DATA_BUCKET_URI = f\"gs://{PROJECT}-{DATA_BUCKET_NAME}\"\n",
    "PROCESSED_DATA_SAVE_BUCKET_URI = f\"gs://{PROJECT}-{PROCESSED_DATA_SAVE_BUCKET_NAME}\"\n",
    "PROCESSED_DATA_BUCKET_URI = f\"gs://{PROJECT}-{PROCESSED_DATA_BUCKET_NAME}\"\n",
    "NEW_TRAIN_DATA_BUCKET_URI = f\"gs://{PROJECT}-{NEW_TRAIN_DATA_BUCKET_NAME}\"\n",
    "VALID_DATA_BUCKET_URI = f\"gs://{PROJECT}-{VALID_DATA_BUCKET_NAME}\"\n",
    "\n",
    "MODEL_BUCKET_URI = f\"gs://{PROJECT}-{MODEL_BUCKET_NAME}\"\n",
    "\n",
    "# gar repo name\n",
    "REPO_NAME = os.environ[\"REPO_NAME\"]\n",
    "\n",
    "# docker image names\n",
    "DATA_VALIDATION_IMAGE_NAME = os.environ[\"DATA_VALIDATION_IMAGE_NAME\"]\n",
    "DATA_PROCESSING_IMAGE_NAME = os.environ[\"DATA_PROCESSING_IMAGE_NAME\"]\n",
    "HP_TUNING_IMAGE_NAME = os.environ[\"HP_TUNING_IMAGE_NAME\"]\n",
    "TRAINING_IMAGE_NAME = os.environ[\"TRAINING_IMAGE_NAME\"]\n",
    "FINE_TUNING_IMAGE_NAME = os.environ[\"FINE_TUNING_IMAGE_NAME\"]\n",
    "\n",
    "# docker images\n",
    "DATA_VALIDATION_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPO_NAME}/{DATA_VALIDATION_IMAGE_NAME}:latest\"\n",
    "DATA_PROCESSING_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPO_NAME}/{DATA_PROCESSING_IMAGE_NAME}:latest\"\n",
    "HP_TUNING_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPO_NAME}/{HP_TUNING_IMAGE_NAME}:latest\"\n",
    "TRAINING_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPO_NAME}/{TRAINING_IMAGE_NAME}:latest\"\n",
    "FINE_TUNING_IMAGE = f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPO_NAME}/{FINE_TUNING_IMAGE_NAME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d9b0b9-eafb-4851-8f1c-bce0e79b6556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import compiler, dsl\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from google_cloud_pipeline_components.v1 import custom_job, endpoint, model\n",
    "\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93043b6c-e6d1-4ef6-b4a9-7c98fa034e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIPELINE_BUCKET_NAME = os.environ[\"PIPELINE_BUCKET_NAME\"]\n",
    "PIPELINE_BUCKET_URI = f\"gs://{PROJECT}-{PIPELINE_BUCKET_NAME}\"\n",
    "\n",
    "aip.init(project=PROJECT, staging_bucket=PIPELINE_BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3af450f-70bf-49ea-a538-ed3ddfc518a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11.8-slim-bookworm\",\n",
    "               packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def data_validation(\n",
    "    data_bucket_uri: str,\n",
    "    data_validation_image: str,\n",
    "    staging_bucket_uri: str,\n",
    "    location: str,\n",
    "    project: str\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    data_validation_container_env_variables = [{ \"name\": \"data_bucket_ur\", \"value\": data_bucket_uri }]\n",
    "    \n",
    "    data_validation_worker_pool_specs = [\n",
    "        { \"container_spec\": { \"image_uri\": data_validation_image, \n",
    "                              \"env\": data_validation_container_env_variables },         \n",
    "          \"replica_count\": 1,                            \n",
    "          \"machine_spec\": { \"machine_type\": \"e2-highmem-2\" } }\n",
    "    ]\n",
    "    \n",
    "    data_validation_job = aiplatform.CustomJob(\n",
    "        display_name=\"data-validation-component\",                     \n",
    "        location=location,                       \n",
    "        project=project,\n",
    "        worker_pool_specs=data_validation_worker_pool_specs,     \n",
    "        staging_bucket=staging_bucket_uri,\n",
    "    )\n",
    "    \n",
    "    data_validation_job.run(timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559d8e14-7ebb-4790-a4ec-95032cd3f280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11.8-slim-bookworm\",\n",
    "               packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def data_processing(\n",
    "    data_bucket_uri: str,\n",
    "    processed_data_save_bucket_uri: str,\n",
    "    fraction_for_valid_and_test_data: str,\n",
    "    data_processing_image: str,\n",
    "    staging_bucket_uri: str,\n",
    "    location: str,\n",
    "    project: str\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    data_processing_container_env_variables = [\n",
    "        { \"name\": \"data_bucket_uri\", \"value\": data_bucket_uri },\n",
    "        { \"name\": \"processed_data_save_bucket\", \"value\": processed_data_save_bucket_uri },\n",
    "        { \"name\": \"fraction_for_valid_and_test_data\", \"value\": fraction_for_valid_and_test_data }\n",
    "    ]\n",
    "    \n",
    "    data_processing_worker_pool_specs = [\n",
    "        { \"container_spec\": { \"image_uri\": data_processing_image,\n",
    "                              \"env\": data_processing_container_env_variables },\n",
    "          \"replica_count\": 1,\n",
    "          \"machine_spec\": { \"machine_type\": \"e2-highmem-2\" } }\n",
    "    ]\n",
    "    \n",
    "    data_processing_job = aiplatform.CustomJob(\n",
    "        display_name=\"data-processing-component\",                                \n",
    "        location=location,\n",
    "        project=project,                        \n",
    "        worker_pool_specs=data_processing_worker_pool_specs,\n",
    "        staging_bucket=staging_bucket_uri\n",
    "    )\n",
    "    \n",
    "    data_processing_job.run(timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e51ad20-af3e-4bac-ba09-85761398ebc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11.8-slim-bookworm\",\n",
    "               packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def hyperparameter_tuning(\n",
    "    max_trials: str, \n",
    "    hp_epochs: str,\n",
    "    hp_tuning_image: str,\n",
    "    staging_bucket_uri: str,\n",
    "    location: str,\n",
    "    project: str\n",
    "):\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    hp_tuning_container_env_variables = [\n",
    "        { \"name\": \"learning_rating\", \"value\": \"0.0001\" },\n",
    "        { \"name\": \"number_of_layers\", \"value\": \"4\" },\n",
    "        { \"name\": \"max_trials\", \"value\": max_trials },\n",
    "        { \"name\": \"epochs\", \"value\": hp_epochs }\n",
    "    ]\n",
    "\n",
    "    hpt_worker_pool_specs = [\n",
    "        { \"container_spec\": { \"image_uri\": hp_tuning_image, \"env\": hp_tuning_container_env_variables },\n",
    "          \"replica_count\": 1,\n",
    "          \"machine_spec\": { \"machine_type\": \"e2-highmem-2\" } }\n",
    "    ]\n",
    "\n",
    "    hp_tuning_job = aiplatform.CustomJob(\n",
    "        display_name=\"hyperparameter-tuning-component\",                          \n",
    "        location=location,\n",
    "        project=project,\n",
    "        worker_pool_specs=hpt_worker_pool_specs,\n",
    "        staging_bucket=staging_bucket_uri\n",
    "    )\n",
    "\n",
    "    hp_tuning_job.run(timeout=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c1e51f-c15d-4500-b93d-a0aa4b41e62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11.8-slim-bookworm\",\n",
    "               packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_training(\n",
    "    train_epochs: str,\n",
    "    training_image: str,\n",
    "    staging_bucket_uri: str,\n",
    "    location: str,\n",
    "    project: str\n",
    "):\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    training_container_env_variables = [{ \"name\": \"epochs\", \"value\": train_epochs }]\n",
    "    \n",
    "    training_worker_pool_specs = [\n",
    "        { \"container_spec\": { \"image_uri\": training_image, \"env\": training_container_env_variables },\n",
    "          \"replica_count\": 1,\n",
    "          \"machine_spec\": { \"machine_type\": \"e2-highmem-2\" } }\n",
    "    ]\n",
    "    \n",
    "    training_job = aiplatform.CustomJob(\n",
    "        display_name=\"training-component\",\n",
    "        location=location,\n",
    "        project=project,\n",
    "        worker_pool_specs=training_worker_pool_specs,\n",
    "        staging_bucket=staging_bucket_uri\n",
    "    )\n",
    "    \n",
    "    training_job.run(timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46dd5f7-b770-453f-b209-2933651e4ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@dsl.component(base_image=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-13.py310:latest\",\n",
    "               packages_to_install=[\"pandas\", \"numpy\", \"fsspec\", \"gcsfs\"])\n",
    "def eval_and_deploy_decision(\n",
    "    model_bucket_uri: str,\n",
    "    valid_data_bucket_uri: str,\n",
    "    mae_threshold: float\n",
    ") -> NamedTuple(\"Outputs\", [(\"deploy_decision\", bool)]):\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"Attempting to load trained and saved model ...\")\n",
    "    saved_model_path = model_bucket_uri + \"/model_artifacts\"\n",
    "    \n",
    "    print(\"saved_model_path:\", saved_model_path)\n",
    "    loaded_model_artifacts = tf.saved_model.load(export_dir=saved_model_path)\n",
    "    print(\"Model successfully loaded\")\n",
    "    \n",
    "    model = loaded_model_artifacts.signatures[\"serving_default\"]\n",
    "    print(\"serving function successfully loaded\")\n",
    "    \n",
    "    valid_data_path = valid_data_bucket_uri + \"/valid.csv\"\n",
    "    print(\"Attempting to read the data at:\", valid_data_path)\n",
    "    \n",
    "    x_valid = pd.read_csv(valid_data_path)\n",
    "    y_valid = x_valid.pop(\"log_price\")\n",
    "    \n",
    "    print(\"Read successful\")\n",
    "\n",
    "    valid_float_inputs = [list(row) for row in x_valid.values[:, :12]]\n",
    "    valid_string_inputs = [list(row) for row in x_valid.values[:, 12:]]\n",
    "\n",
    "    predictions_dict = model(float_inputs=valid_float_inputs, string_inputs=valid_string_inputs)\n",
    "\n",
    "    key = list(predictions_dict.keys())[0]\n",
    "    \n",
    "    predictions_valid = tf.squeeze(predictions_dict[key], axis=-1)\n",
    "    log_predictions_valid = tf.math.log(predictions_valid)\n",
    "    \n",
    "    log_y_true_valid = tf.constant(y_valid, dtype=tf.float32)\n",
    "    \n",
    "    mean_absolute_error = tf.math.reduce_mean(tf.abs(log_predictions_valid - log_y_true_valid))\n",
    "\n",
    "    print(\"mean_absolute_error:\", mean_absolute_error)\n",
    "    \n",
    "    if mean_absolute_error.numpy() < mae_threshold:\n",
    "        deploy_decision = True\n",
    "    else:\n",
    "        deploy_decision = False\n",
    "        \n",
    "    return (deploy_decision,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd7e595-e978-447e-855a-2f0446e71962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_deployment_components(deploy_decision_task):\n",
    "    \n",
    "    # generic component for putting saved model artifacts into a kubeflow pipeline component\n",
    "    model_save_uri = MODEL_BUCKET_URI + \"/model_artifacts\"\n",
    "    serving_image = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-13:latest\"\n",
    "    \n",
    "    import_unmanaged_model_task = dsl.importer(artifact_uri=model_save_uri,\n",
    "                                               artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "                                               metadata={ \"containerSpec\": { \"imageUri\": serving_image } })\n",
    " \n",
    "    import_unmanaged_model_task.after(deploy_decision_task)\n",
    "    \n",
    "    # component for uploading a model to Vertex AI model registry\n",
    "    model_upload_task = model.ModelUploadOp(project=PROJECT,\n",
    "                                            display_name=\"used-cars-model\",\n",
    "                                            location=LOCATION,\n",
    "                                            unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"])\n",
    "    \n",
    "    model_upload_task.after(import_unmanaged_model_task)\n",
    "    \n",
    "    # component for creating the model serving endpoint on Vertex AI\n",
    "    create_endpoint_task = endpoint.EndpointCreateOp(project=PROJECT,\n",
    "                                                     location=LOCATION,\n",
    "                                                     display_name=\"used-cars-model-inference-endpoint\")\n",
    "    \n",
    "    # component for deploying the model on Vertex AI\n",
    "    endpoint.ModelDeployOp(model=model_upload_task.outputs[\"model\"],\n",
    "                           endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "                           dedicated_resources_min_replica_count=1,\n",
    "                           dedicated_resources_max_replica_count=1,\n",
    "                           dedicated_resources_machine_type=\"e2-standard-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5954a70-baa5-4f09-a67b-bcc9ca983b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11.8-slim-bookworm\",\n",
    "               packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def model_fine_tuning(\n",
    "    fine_tuning_epochs: str,\n",
    "    fine_tuning_learning_rate: str, \n",
    "    new_train_data_bucket_uri: str,\n",
    "    valid_data_bucket_uri: str,\n",
    "    fine_tuning_image: str,\n",
    "    staging_bucket_uri: str,\n",
    "    location: str,\n",
    "    project: str\n",
    "):\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    fine_tuning_container_env_variables = [\n",
    "        { \"name\": \"epochs\", \"value\": fine_tuning_epochs },\n",
    "        { \"name\": \"learning_rate\", \"value\": fine_tuning_learning_rate },\n",
    "        { \"name\": \"new_train_data_bucket\", \"value\": new_train_data_bucket_uri },\n",
    "        { \"name\": \"valid_data_bucket\", \"value\": valid_data_bucket_uri }\n",
    "    ]\n",
    "    \n",
    "    fine_tuning_worker_pool_specs = [\n",
    "        { \"container_spec\": { \"image_uri\": fine_tuning_image,\n",
    "                             \"env\": fine_tuning_container_env_variables },\n",
    "          \"replica_count\": 1,\n",
    "          \"machine_spec\": { \"machine_type\": \"e2-highmem-2\" } }\n",
    "    ]\n",
    "    \n",
    "    fine_tuning_job = aiplatform.CustomJob(\n",
    "        display_name=\"fine-tuning-component\",\n",
    "        location=location,\n",
    "        project=project,\n",
    "        worker_pool_specs=fine_tuning_worker_pool_specs,\n",
    "        staging_bucket=staging_bucket_uri\n",
    "    )\n",
    "    \n",
    "    fine_tuning_job.run(timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d321c619-af31-45d1-8587-bd5ede37c743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(base_image=\"python:3.11\")\n",
    "def model_evaluation_failed(failure_message: str):\n",
    "    print(failure_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca63aaa9-f760-4620-8775-1ec74cb6b28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "@dsl.pipeline(name=\"kubeflow-pipeline-used-cars\",\n",
    "              description=\"A kubeflow pipeline for used cars project\",\n",
    "              pipeline_root=PIPELINE_BUCKET_URI)\n",
    "def pipeline_function(\n",
    "    first_time_training: bool = False,\n",
    "    location: str = LOCATION,\n",
    "    project: str = PROJECT,\n",
    "    staging_bucket_uri: str = PIPELINE_BUCKET_URI,\n",
    "    data_bucket_uri: str = DATA_BUCKET_URI,\n",
    "    processed_data_save_bucket_uri: str = PROCESSED_DATA_SAVE_BUCKET_URI,\n",
    "    processed_data_bucket_uri: str = PROCESSED_DATA_BUCKET_URI,\n",
    "    new_train_data_bucket_uri: str = NEW_TRAIN_DATA_BUCKET_URI,\n",
    "    valid_data_bucket_uri: str = VALID_DATA_BUCKET_URI,\n",
    "    model_bucket_uri: str = MODEL_BUCKET_URI,\n",
    "    data_validation_image: str = DATA_VALIDATION_IMAGE,\n",
    "    data_processing_image: str = DATA_PROCESSING_IMAGE,\n",
    "    hp_tuning_image: str = HP_TUNING_IMAGE,\n",
    "    training_image: str = TRAINING_IMAGE,\n",
    "    fine_tuning_image: str = FINE_TUNING_IMAGE,\n",
    "    fraction_for_valid_and_test_data: str = \"0.2\",\n",
    "    max_trials: str = \"3\",\n",
    "    hp_epochs: str = \"1\",\n",
    "    train_epochs: str = \"5\",\n",
    "    mae_threshold: float = 5.0,\n",
    "    fine_tuning_epochs: str = \"3\",\n",
    "    fine_tuning_learning_rate: str = \"0.0005\",\n",
    "):\n",
    "\n",
    "    data_validation_task = data_validation(\n",
    "        data_bucket_uri=data_bucket_uri,\n",
    "        data_validation_image=data_validation_image,\n",
    "        staging_bucket_uri=staging_bucket_uri,\n",
    "        location=location,\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    data_processing_task = data_processing(\n",
    "        data_bucket_uri=data_bucket_uri,\n",
    "        processed_data_save_bucket_uri=processed_data_save_bucket_uri,\n",
    "        fraction_for_valid_and_test_data=fraction_for_valid_and_test_data,\n",
    "        data_processing_image=data_processing_image,\n",
    "        staging_bucket_uri=staging_bucket_uri,\n",
    "        location=location,\n",
    "        project=project\n",
    "    )\n",
    "\n",
    "    data_processing_task.after(data_validation_task)\n",
    "    \n",
    "    with dsl.If(first_time_training == True, name=\"training-from-scratch\"):\n",
    "        \n",
    "        hp_tuning_task = hyperparameter_tuning(\n",
    "            max_trials=max_trials, \n",
    "            hp_epochs=hp_epochs,\n",
    "            hp_tuning_image=hp_tuning_image,\n",
    "            staging_bucket_uri=staging_bucket_uri,\n",
    "            location=location,\n",
    "            project=project\n",
    "        )\n",
    "        \n",
    "        hp_tuning_task.after(data_processing_task)\n",
    "        \n",
    "        training_task = model_training(\n",
    "            train_epochs=train_epochs,\n",
    "            training_image=training_image,\n",
    "            staging_bucket_uri=staging_bucket_uri,\n",
    "            location=location,\n",
    "            project=project\n",
    "        )\n",
    "        \n",
    "        training_task.after(hp_tuning_task)\n",
    "\n",
    "        deploy_decision_task = eval_and_deploy_decision(\n",
    "            model_bucket_uri=model_bucket_uri,\n",
    "            valid_data_bucket_uri=valid_data_bucket_uri,\n",
    "            mae_threshold=mae_threshold\n",
    "        )\n",
    "        \n",
    "        deploy_decision_task.after(training_task)\n",
    "        \n",
    "        with dsl.If(deploy_decision_task.outputs[\"deploy_decision\"] == True, name=\"deploy decision: YES\"):\n",
    "            \n",
    "            model_deployment_components(deploy_decision_task)\n",
    "            \n",
    "        with dsl.Else(name=\"deploy decision: NO\"):\n",
    "            \n",
    "            failure_message = \"Model's hyperparameters were tuned and trained, but still was not performant enough and failed its evaluation\"\n",
    "            model_eval_failed_task = model_evaluation_failed(failure_message=failure_message)\n",
    "            \n",
    "            model_eval_failed_task.after(deploy_decision_task)\n",
    "   \n",
    "    with dsl.Else(name=\"model-fine-tuning\"):\n",
    "        \n",
    "        fine_tuning_task = model_fine_tuning(\n",
    "            fine_tuning_epochs=fine_tuning_epochs,\n",
    "            fine_tuning_learning_rate=fine_tuning_learning_rate,\n",
    "            new_train_data_bucket_uri=new_train_data_bucket_uri,\n",
    "            valid_data_bucket_uri=valid_data_bucket_uri,\n",
    "            fine_tuning_image=fine_tuning_image,\n",
    "            staging_bucket_uri=staging_bucket_uri,\n",
    "            location=location,\n",
    "            project=project\n",
    "        )\n",
    "        \n",
    "        fine_tuning_task.after(data_processing_task)\n",
    "\n",
    "        deploy_decision_task = eval_and_deploy_decision(\n",
    "            model_bucket_uri=model_bucket_uri,\n",
    "            valid_data_bucket_uri=valid_data_bucket_uri,\n",
    "            mae_threshold=mae_threshold\n",
    "        )\n",
    "        \n",
    "        deploy_decision_task.after(fine_tuning_task)\n",
    "        \n",
    "        with dsl.If(deploy_decision_task.outputs[\"deploy_decision\"] == True, name=\"deploy decision: YES\"):\n",
    "            \n",
    "            model_deployment_components(deploy_decision_task)\n",
    "            \n",
    "        with dsl.Else(name=\"deploy decision: NO\"):\n",
    "            \n",
    "            failure_message = \"Previously trained model was fine tuned but the fine tuned model was not as performant as the previous model\"\n",
    "            model_eval_failed_task = model_evaluation_failed(failure_message=failure_message)\n",
    "            \n",
    "            model_eval_failed_task.after(deploy_decision_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "626a14d2-eb75-47d2-9d00-013dd00cacc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline_function, package_path=\"used_cars_kubeflow_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c094eb91-3482-455d-a83e-a8e6d219474c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_values = { \n",
    "    \"first_time_training\": True,\n",
    "    \"location\": LOCATION,\n",
    "    \"project\": PROJECT,\n",
    "    \"staging_bucket_uri\": PIPELINE_BUCKET_URI,\n",
    "    \"data_bucket_uri\": DATA_BUCKET_URI,\n",
    "    \"processed_data_save_bucket_uri\": PROCESSED_DATA_SAVE_BUCKET_URI,\n",
    "    \"processed_data_bucket_uri\": PROCESSED_DATA_BUCKET_URI,\n",
    "    \"new_train_data_bucket_uri\": NEW_TRAIN_DATA_BUCKET_URI,\n",
    "    \"valid_data_bucket_uri\": VALID_DATA_BUCKET_URI,\n",
    "    \"model_bucket_uri\": MODEL_BUCKET_URI,\n",
    "    \"data_validation_image\": DATA_VALIDATION_IMAGE,\n",
    "    \"data_processing_image\": DATA_PROCESSING_IMAGE,\n",
    "    \"hp_tuning_image\": HP_TUNING_IMAGE,\n",
    "    \"training_image\": TRAINING_IMAGE,\n",
    "    \"fine_tuning_image\": FINE_TUNING_IMAGE,\n",
    "    \"fraction_for_valid_and_test_data\": \"0.2\",\n",
    "    \"max_trials\": \"3\",\n",
    "    \"hp_epochs\": \"5\",\n",
    "    \"train_epochs\": \"25\",\n",
    "    \"mae_threshold\": 5.0,\n",
    "    \"fine_tuning_epochs\": \"3\",\n",
    "    \"fine_tuning_learning_rate\": \"0.0005\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8af431d6-f290-43fe-96ec-7e742eef4e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/kubeflow-pipeline-used-cars-20240519035654?project=825348564081\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/825348564081/locations/us-central1/pipelineJobs/kubeflow-pipeline-used-cars-20240519035654\n"
     ]
    }
   ],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=\"used_cars_kubeflow_pipeline_job\",\n",
    "    template_path=\"used_cars_kubeflow_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_BUCKET_URI,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values\n",
    ")\n",
    "                                    \n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ebfa67-8a2f-43ba-a0dd-f4bc9f455bec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[34733.9844], [16445.4746], [23626.3965]], deployed_model_id='4540917052017213440', metadata=None, model_version_id='1', model_resource_name='projects/825348564081/locations/us-east1/models/5816917888237305856', explanations=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "PROJECT_ID = 825348564081\n",
    "ENDPOINT_ID = 7919654711521705984\n",
    "\n",
    "endpoint = aiplatform.Endpoint(\n",
    "    endpoint_name=f\"projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/{ENDPOINT_ID}\"\n",
    ")\n",
    "\n",
    "instances = [\n",
    "      { \n",
    "        \"float_inputs\": [5.0, 20.0, 95.0, 2021.0, 5.0, 5.0, 4.9, 4.8, 4.8, 5.0, 30.28433366877628, 23.5],\n",
    "        \"string_inputs\": ['Honda', 'Front-wheel Drive', 'None reported', ' Yes', ' Yes', ' No', 'At least 1 recall'] \n",
    "      },\n",
    "      {\n",
    "        \"float_inputs\": [3.5, 6.0, 50.0, 2017.0, 3.7, 4.2, 3.3, 3.3, 4.0, 3.2, 46.48263119767776, 24.0],\n",
    "        \"string_inputs\": ['RAM', 'Front-wheel Drive', 'None reported', ' No', ' No', ' Unknown', 'Unknown']\n",
    "      },\n",
    "      {\n",
    "        \"float_inputs\": [4.8, 277.0, 94.0, 2020.0, 4.9, 4.8, 4.8, 4.8, 4.9, 4.9, 29.127461156199114, 29.5],\n",
    "        \"string_inputs\": ['Honda', 'All-wheel Drive', 'None reported', ' Yes', ' Yes', ' Unknown', 'Unknown']\n",
    "      }\n",
    "]\n",
    "\n",
    "endpoint.predict(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28d15b-3430-4bcb-ac43-fbb13fde2f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
